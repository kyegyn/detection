import torch
import numpy as np
from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, roc_curve, confusion_matrix
import matplotlib.pyplot as plt


class BinaryMetrics:
    """
    äºŒåˆ†ç±»æŒ‡æ ‡è®¡ç®—å™¨
    è‡ªåŠ¨ç´¯ç§¯éªŒè¯é›†çš„æ‰€æœ‰ Batchï¼Œå¹¶åœ¨ Epoch ç»“æŸæ—¶è®¡ç®— AUC, EER, F1 ç­‰å…³é”®æŒ‡æ ‡ã€‚
    """
    def __init__(self):
        self.reset()

    def reset(self):
        """æ¸…ç©ºç¼“å­˜ï¼Œå¼€å§‹æ–°ä¸€è½®è¯„ä¼°"""
        self.preds = []   # å­˜å‚¨æ¦‚ç‡å€¼ (0.0 - 1.0)
        self.targets = [] # å­˜å‚¨çœŸå®æ ‡ç­¾ (0 æˆ– 1)

    def update(self, logits, labels):
        """
        åœ¨æ¯ä¸ª Batch ç»“æŸåè°ƒç”¨
        Args:
            logits: æ¨¡å‹è¾“å‡ºçš„åŸå§‹ Logits [B]
            labels: çœŸå®æ ‡ç­¾ [B]
        """
        # 1. Sigmoid è½¬æ¢ä¸ºæ¦‚ç‡
        probs = torch.sigmoid(logits).detach().cpu().numpy()
        labels = labels.detach().cpu().numpy()

        # 2. å­˜å…¥åˆ—è¡¨
        self.preds.extend(probs)
        self.targets.extend(labels)

    def compute(self):
        """
        è®¡ç®—æ‰€æœ‰ç´¯ç§¯æ•°æ®çš„æŒ‡æ ‡
        Returns:
            metrics_dict: åŒ…å« acc, auc, eer, f1, precision, recall çš„å­—å…¸
        """
        y_true = np.array(self.targets)
        y_score = np.array(self.preds)

        # é»˜è®¤é˜ˆå€¼ 0.5 ç”¨äºè®¡ç®—ç¡¬åˆ†ç±»æŒ‡æ ‡
        y_pred = (y_score > 0.5).astype(int)

        # 1. åŸºç¡€æŒ‡æ ‡
        acc = accuracy_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        prec = precision_score(y_true, y_pred, zero_division=0)
        rec = recall_score(y_true, y_pred, zero_division=0)

        # 2. AUC (Area Under Curve)
        try:
            auc = roc_auc_score(y_true, y_score)
        except ValueError:
            auc = 0.0 # é˜²æ­¢åªæœ‰ä¸€ä¸ªç±»åˆ«æ—¶æŠ¥é”™

        # 3. EER (Equal Error Rate) - å–è¯æ ¸å¿ƒæŒ‡æ ‡
        eer, threshold_eer = self._calculate_eer(y_true, y_score)

        return {
            "Acc": acc,
            "AUC": auc,
            "EER": eer,
            "F1": f1,
            "Precision": prec,
            "Recall": rec,
            "Best_Thresh": threshold_eer # EER å¯¹åº”çš„æœ€ä½³é˜ˆå€¼
        }

    def _calculate_eer(self, y_true, y_score):
        """
        è®¡ç®— EER (Equal Error Rate)
        EER æ˜¯ FAR (False Acceptance Rate) å’Œ FRR (False Rejection Rate) æœ€æ¥è¿‘æ—¶çš„å€¼ã€‚
        """
        fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)

        # FRR = 1 - TPR
        frr = 1 - tpr

        # æ‰¾åˆ° FAR å’Œ FRR å·®å€¼æœ€å°çš„ç‚¹
        abs_diffs = np.abs(fpr - frr)
        min_index = np.argmin(abs_diffs)

        eer = (fpr[min_index] + frr[min_index]) / 2
        best_threshold = thresholds[min_index]

        return eer, best_threshold

    def print_report(self):
        """æ‰“å°æ ¼å¼åŒ–çš„æŠ¥å‘Š"""
        res = self.compute()
        print("-" * 30)
        print(f"ğŸ“Š Evaluation Report:")
        print(f"Accuracy : {res['Acc']:.4f}")
        print(f"AUC      : {res['AUC']:.4f}")
        print(f"EER      : {res['EER']:.4f} (Lower is better)")
        print(f"F1-Score : {res['F1']:.4f}")
        print(f"Precision: {res['Precision']:.4f}")
        print(f"Recall   : {res['Recall']:.4f}")
        print("-" * 30)
        return res

    def plot_roc(self, save_path=None):
        """ç”»å‡º ROC æ›²çº¿å¹¶ä¿å­˜ (å¯é€‰)"""
        y_true = np.array(self.targets)
        y_score = np.array(self.preds)
        fpr, tpr, _ = roc_curve(y_true, y_score)
        auc = roc_auc_score(y_true, y_score)

        plt.figure()
        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc:.2f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver Operating Characteristic')
        plt.legend(loc="lower right")

        if save_path:
            plt.savefig(save_path)
            plt.close()
        else:
            plt.show()
