import logging
import os
import sys
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime


class ExperimentLogger:
    """
    ç»Ÿä¸€æ—¥å¿—ç®¡ç†ç±»ï¼šåŒæ—¶å¤„ç† æ§åˆ¶å°è¾“å‡ºã€æ–‡æœ¬æ–‡ä»¶è®°å½• å’Œ TensorBoard å¯è§†åŒ–
    """

    def __init__(self, log_dir, experiment_name=None):
        if experiment_name is None:
            experiment_name = datetime.now().strftime("%Y%m%d_%H%M%S")

        self.save_dir = os.path.join(log_dir, experiment_name)
        self.tb_dir = os.path.join(self.save_dir, 'tensorboard')
        os.makedirs(self.save_dir, exist_ok=True)
        os.makedirs(self.tb_dir, exist_ok=True)

        # --- 1. åˆå§‹åŒ– Python æ ‡å‡† Logger ---
        self.logger = logging.getLogger("TSF-Net")
        self.logger.setLevel(logging.INFO)
        self.logger.handlers = []  # é˜²æ­¢é‡å¤æ·»åŠ  handler

        # æ ¼å¼ï¼š[æ—¶é—´] [çº§åˆ«] æ¶ˆæ¯
        formatter = logging.Formatter('[%(asctime)s] [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

        # Handler A: è¾“å‡ºåˆ°æ–‡ä»¶
        fh = logging.FileHandler(os.path.join(self.save_dir, 'train.log'), encoding='utf-8')
        fh.setFormatter(formatter)
        self.logger.addHandler(fh)

        # Handler B: è¾“å‡ºåˆ°æ§åˆ¶å° (Stream)
        sh = logging.StreamHandler(sys.stdout)
        sh.setFormatter(formatter)
        self.logger.addHandler(sh)

        # --- 2. åˆå§‹åŒ– TensorBoard ---
        self.writer = SummaryWriter(log_dir=self.tb_dir)

        self.logger.info(f"ğŸš€ Experiment initialized at: {self.save_dir}")

    def log_hyperparams(self, config):
        """è®°å½•è¶…å‚æ•°é…ç½®"""
        self.logger.info("=== Hyperparameters ===")
        for k, v in config.items():
            self.logger.info(f"{k}: {v}")
        self.logger.info("=======================")

    def log_step(self, epoch, step, global_step, losses_dict):
        """
        Step çº§åˆ«çš„æ—¥å¿— (é€šå¸¸åªå†™ TensorBoardï¼Œé˜²æ­¢æ–‡æœ¬æ—¥å¿—çˆ†ç‚¸)
        Args:
            losses_dict: {'total': 1.5, 'bce': 0.8, 'supcon': 0.7}
        """
        for name, value in losses_dict.items():
            self.writer.add_scalar(f'Train_Step/{name}', value, global_step)

    def log_epoch(self, epoch, train_metrics, val_metrics, lr):
        """
        Epoch çº§åˆ«çš„æ—¥å¿— (è®°å½•åˆ°æ–‡æœ¬ + TensorBoard)
        """
        # 1. è®°å½•æ–‡æœ¬
        msg = f"Epoch [{epoch}] | LR: {lr:.6f} | "
        msg += f"Train Loss: {train_metrics['loss']:.4f} | "
        msg += f"Val Acc: {val_metrics['Acc']:.4f} | Val AUC: {val_metrics['AUC']:.4f} | Val EER: {val_metrics['EER']:.4f}"
        self.logger.info(msg)

        # 2. è®°å½• TensorBoard - Train
        for k, v in train_metrics.items():
            self.writer.add_scalar(f'Train_Epoch/{k}', v, epoch)

        # 3. è®°å½• TensorBoard - Val
        for k, v in val_metrics.items():
            self.writer.add_scalar(f'Val_Epoch/{k}', v, epoch)

        # 4. è®°å½• LR
        self.writer.add_scalar('Hyperparams/Learning_Rate', lr, epoch)

    def log_info(self, msg):
        """é€šç”¨ info è®°å½•"""
        self.logger.info(msg)

    def close(self):
        """å…³é—­èµ„æº"""
        self.writer.close()
        # ç§»é™¤ handlers é˜²æ­¢å†…å­˜æ³„æ¼
        for handler in self.logger.handlers:
            handler.close()
            self.logger.removeHandler(handler)
