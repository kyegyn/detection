import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import CLIPVisionModel
# å¼•å…¥ LoRA åº“
from peft import get_peft_model, LoraConfig
import logging


class SemanticBranch(nn.Module):
    """
    æ”¯è·¯ä¸€ï¼šåŸºäº CLIP çš„è¯­ä¹‰æå–ä¸æ˜ å°„æ¨¡å—
    åŠŸèƒ½ï¼šå†…éƒ¨ç®¡ç† CLIP æ¨¡å‹ï¼ˆåŠ è½½ã€å†»ç»“ã€LoRAå¾®è°ƒï¼‰ï¼Œå¹¶æŠ•å½±åˆ°å¯¹æ¯”å­¦ä¹ ç©ºé—´ã€‚
    """

    def __init__(self, config):
        """
        config: åŒ…å«æ‰€æœ‰æ¨¡å‹é…ç½®çš„å­—å…¸ï¼Œä¾‹å¦‚ 'clip_model', 'projection_dim', 'use_lora' ç­‰
        """
        super(SemanticBranch, self).__init__()
        self.logger = logging.getLogger("TSF-Net")

        model_name = config.get('clip_model', "openai/clip-vit-base-patch32")
        projection_dim = config.get('projection_dim', 256)

        self.logger.info(f"ğŸ”„ Loading CLIP Vision Model: {model_name} inside SemanticBranch...")

        # 1. åŠ è½½ CLIP (Vision Tower Only)
        self.clip_v = CLIPVisionModel.from_pretrained(model_name)
        self.clip_dim = self.clip_v.config.hidden_size

        # 2. è®­ç»ƒç­–ç•¥ï¼šLoRA vs è§£å†»å¾®è°ƒ
        if config.get('use_lora', False):
            # --- A. LoRA æ¨¡å¼ ---
            self.logger.info("ğŸ”§ Applying LoRA to CLIP...")
            lora_config = LoraConfig(
                r=config.get('lora_r', 8),
                lora_alpha=config.get('lora_alpha', 16),
                # Hugging Face CLIP çš„ Attention å±‚å‘½åé€šå¸¸æ˜¯ q_proj, v_proj
                target_modules=["q_proj", "v_proj"],
                lora_dropout=0.1,
                bias="none"
            )
            # ä½¿ç”¨ peft åŒ…è£…æ¨¡å‹ï¼Œè¿™å°†è‡ªåŠ¨å†»ç»“é LoRA å‚æ•°
            self.clip_v = get_peft_model(self.clip_v, lora_config)
            self.clip_v.print_trainable_parameters()

        else:
            # --- B. ä¼ ç»Ÿå¾®è°ƒæ¨¡å¼ (è§£å†»æœ€åä¸€å±‚) ---
            self.logger.info("â„ï¸ Freezing CLIP backbone initially...")
            self.freeze_backbone()

            # é»˜è®¤è§£å†»æœ€å 1 å±‚ Block + LayerNorm
            self.logger.info("ğŸ”“ Unfreezing last visual block for fine-tuning...")
            self.unfreeze_backbone(last_n_layers=1)

        # 3. æ˜ å°„æ¨¡å— (Projector)
        # ä¿æŒä½ åŸæœ‰çš„éçº¿æ€§ç“¶é¢ˆç»“æ„
        self.projector = nn.Sequential(
            nn.Linear(self.clip_dim, self.clip_dim),
            nn.LayerNorm(self.clip_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(self.clip_dim, projection_dim),
            nn.LayerNorm(projection_dim)
        )

    def freeze_backbone(self):
        for param in self.clip_v.parameters():
            param.requires_grad = False

    def unfreeze_backbone(self, last_n_layers=None):
        """
        è§£å†»ç­–ç•¥ï¼šé’ˆå¯¹ Hugging Face CLIP Vision Model çš„ç»“æ„
        """
        # 1. å¿…é¡»è¦è§£å†»æœ€åçš„ LayerNorm (post_layernorm)
        if hasattr(self.clip_v, "vision_model") and hasattr(self.clip_v.vision_model, "post_layernorm"):
            for param in self.clip_v.vision_model.post_layernorm.parameters():
                param.requires_grad = True

        # 2. è§£å†» Transformer Layers
        if last_n_layers is None:
            # å…¨éƒ¨è§£å†»
            for param in self.clip_v.parameters():
                param.requires_grad = True
        else:
            # åªè§£å†»æœ€å N å±‚
            # è·¯å¾„: vision_model.encoder.layers
            if hasattr(self.clip_v, "vision_model") and hasattr(self.clip_v.vision_model, "encoder"):
                layers = self.clip_v.vision_model.encoder.layers
                total_layers = len(layers)
                for i in range(total_layers - last_n_layers, total_layers):
                    for param in layers[i].parameters():
                        param.requires_grad = True

    def forward(self, pixel_values):
        """
        Args:
            pixel_values: å›¾åƒ Tensor [B, 3, 224, 224]
        """
        # 1. CLIP æå–ç‰¹å¾
        # å¦‚æœä½¿ç”¨äº† LoRAï¼Œè¿™é‡Œå¿…é¡»ä¼ é€’ pixel_values è®©æ¢¯åº¦å›æµï¼Œä¸èƒ½ç”¨é¢„æå–çš„ç‰¹å¾
        outputs = self.clip_v(pixel_values=pixel_values)
        raw_embed = outputs.pooler_output  # [B, clip_dim]

        # 2. æ˜ å°„
        f_semantic = self.projector(raw_embed)

        # 3. å½’ä¸€åŒ– (ç”¨äº SupCon)
        z = F.normalize(f_semantic, p=2, dim=1)

        return z, f_semantic
