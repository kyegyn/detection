import torch
import torch.nn as nn

# å¯¼å…¥æ‰€æœ‰å­æ¨¡å—
from .branches.semantic import SemanticBranch
from .branches.local_patch import LocalPatchBranch
from .branches.global_freq import GlobalFreqBranch
from .fusion import CrossAttentionFusion, FinalClassifier, DiscrepancyFusion, GatingFusion


class TSFNet(nn.Module):
    """
    Tri-Stream Forensic Network (TSF-Net) ä¸»æ¨¡å‹
    """

    def __init__(self, config):
        """
        Args:
            config: å­—å…¸ï¼ŒåŒ…å« 'clip_model', 'use_lora', 'lora_r' ç­‰æ‰€æœ‰é…ç½®
        """
        super(TSFNet, self).__init__()
        self.config = config
        # --- 1. å®ä¾‹åŒ–ä¸‰å¤§æ”¯è·¯ ---
        # æ”¯è·¯ä¸€ï¼šè¯­ä¹‰æµ (CLIP) - å†…éƒ¨å¤„ç†åŠ è½½ä¸ LoRA
        self.branch1 = SemanticBranch(config)
        embed_dim = config['embed_dim']
        # æ”¯è·¯äºŒï¼šå±€éƒ¨çº¹ç†æµ (Patch Shuffle)
        self.branch2 = LocalPatchBranch(
            patch_size=config['patch_size'],
            embed_dim=config['embed_dim']
        )

        # æ”¯è·¯ä¸‰ï¼šå…¨å±€é¢‘ç‡æµ (FFT)
        self.branch3 = GlobalFreqBranch(
            embed_dim=config['embed_dim']
        )

        # --- 2. å®ä¾‹åŒ–èåˆæ¨¡å— ---
        self.fusion = CrossAttentionFusion(
            embed_dim=config['embed_dim'],
            num_heads=8
        )
        # --- 3. é«˜çº§èåˆç­–ç•¥é€‰æ‹© (Switch) ---
        # é»˜è®¤ä¸º 'concat' (è€æ–¹æ³•), å¯é€‰ 'discrepancy' (æƒ…å†µ1), 'gating' (æƒ…å†µ2)
        self.fusion_type = config.get('fusion_type', 'concat')

        if self.fusion_type == 'discrepancy':
            print("ğŸš€ Using Strategy 1: Discrepancy-Aware Fusion")
            self.adv_fusion = DiscrepancyFusion(dim=embed_dim)
            cls_input_dim = embed_dim # èåˆåç»´åº¦ä¿æŒä¸º D

        elif self.fusion_type == 'gating':
            print("ğŸš€ Using Strategy 2: Dynamic Gating Fusion")
            self.adv_fusion = GatingFusion(dim=embed_dim)
            cls_input_dim = embed_dim # èåˆåç»´åº¦ä¿æŒä¸º D

        else:
            print("ğŸš€ Using Default Strategy: Concatenation")
            self.adv_fusion = None
            cls_input_dim = embed_dim + config['projection_dim'] # æ‹¼æ¥åç»´åº¦ D + D


        # --- 3. å®ä¾‹åŒ–åˆ†ç±»å¤´ ---
        self.classifier = FinalClassifier(input_dim=cls_input_dim, hidden_dim=256)

    def forward(self, img):
        """
        Args:
            img: åŸå§‹å›¾åƒ Tensor [B, 3, 224, 224]

        æ³¨æ„ï¼šå½“ä½¿ç”¨ LoRA æ—¶ï¼ŒCLIP æ˜¯å¯è®­ç»ƒçš„ï¼Œå› æ­¤å¿…é¡»è¾“å…¥åŸå§‹å›¾åƒï¼Œ
        ä¸èƒ½å†ä½¿ç”¨ train.py é‡Œé‚£ç§ `with torch.no_grad(): clip(img)` é¢„æå–çš„æ–¹å¼ã€‚
        """

        # --- Step 1: æ”¯è·¯ä¸€ (è¯­ä¹‰) ---
        # z_sem_norm: ç”¨äº SupCon Loss
        # f_sem_raw:  ç”¨äºèåˆ
        z_sem_norm, f_sem_raw = self.branch1(img)

        # --- Step 2: æ”¯è·¯äºŒ (å±€éƒ¨) ---
        f_loc = self.branch2(img)
        f_tex_global = f_loc.mean(dim=1)
        # --- Step 3: æ”¯è·¯ä¸‰ (å…¨å±€) ---
        z_freq = self.branch3(img)

        # --- Step 4: äº¤å‰æ³¨æ„åŠ›èåˆ ---
        v_forensic, attn_weights, x_seq = self.fusion(f_loc, z_freq)

        # =======================================================
        # ã€æ ¸å¿ƒä¿®æ”¹ Step 3.5ã€‘: Modality Dropout (è¯­ä¹‰ä¸¢å¼ƒ)
        # =======================================================
        # é€»è¾‘ï¼šåœ¨è®­ç»ƒæ—¶ï¼Œä»¥ 40% çš„æ¦‚ç‡å°†è¯­ä¹‰ç‰¹å¾å¼ºè¡Œç½®é›¶ã€‚
        # ç›®çš„ï¼šæ¬ºéª—é—¨æ§ç½‘ç»œå’Œåˆ†ç±»å™¨ï¼Œè®©å®ƒä»¬ä»¥ä¸ºè¯­ä¹‰æµå¤±æ•ˆäº†ï¼Œ
        #       ä»è€Œè¢«è¿«å»æŒ–æ˜ v_forensic (ç‰©ç†æµ) ä¸­çš„æœ‰ç”¨ä¿¡æ¯ã€‚
        # f_sem_for_fusion = f_sem_raw

        # if self.training:
        #     # æ¦‚ç‡å»ºè®®è®¾ä¸º 0.3 - 0.5ã€‚è¿™é‡Œè®¾ä¸º 0.4 (40% æ¦‚ç‡ä¸¢å¼ƒè¯­ä¹‰)
        #     if torch.rand(1).item() < 0.5:
        #         f_sem_for_fusion = torch.zeros_like(f_sem_raw)
        # =======================================================
        alpha = None
        # 3. æœ€ç»ˆèåˆå†³ç­– (Strategy Switch)
        if self.fusion_type == 'discrepancy':
            # æƒ…å†µ1ï¼šä¼ å…¥ è¯­ä¹‰å‘é‡ + å–è¯åºåˆ—ç‰¹å¾
            final_feat = self.adv_fusion(f_sem_raw, x_seq)

        elif self.fusion_type == 'gating':
            # æƒ…å†µ2ï¼šä¼ å…¥ è¯­ä¹‰å‘é‡ + å–è¯èšåˆå‘é‡
            final_feat, alpha = self.adv_fusion(f_sem_raw, v_forensic)

        else:
            # é»˜è®¤ï¼šç®€å•æ‹¼æ¥
            final_feat = torch.cat([f_sem_raw, v_forensic], dim=1)
        # --- Step 5: æœ€ç»ˆåˆ†ç±» ---
        logits = self.classifier(final_feat)

        return logits, z_sem_norm, attn_weights, f_sem_raw, v_forensic, alpha, f_tex_global, z_freq
