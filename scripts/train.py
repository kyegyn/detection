import random
import numpy as np
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
from tqdm import tqdm
import os
import yaml
import time
import sys
from PIL import Image
from io import BytesIO
from torch.amp import autocast, GradScaler

# 1. è·å–å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„
current_path = os.path.abspath(__file__)
# 2. è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„ç›®å½•
script_dir = os.path.dirname(current_path)
# 3. è·å–é¡¹ç›®çš„æ ¹ç›®å½•
project_root = os.path.dirname(script_dir)
# 4. å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„ä¸­
if project_root not in sys.path:
    sys.path.append(project_root)

# å¯¼å…¥ä½ ä¹‹å‰å®šä¹‰çš„æ¨¡å—
from models.tsf_net import TSFNet
from data.dataset import ForensicDataset
from losses.supcon_loss import SupConLoss
from utils.fft_utils import seed_everything
from utils.metrics import BinaryMetrics
from utils.logger import ExperimentLogger
from models.fusion import OrthogonalLoss, FineGrainedDecorrelationLoss


# --- 1. å®šä¹‰åœ¨å…¨å±€èŒƒå›´ ---
def worker_init_fn(worker_id):
    worker_seed = torch.initial_seed() % 2 ** 32
    np.random.seed(worker_seed)
    random.seed(worker_seed)


class RandomJPEGCompression(object):
    def __init__(self, quality_range=(50, 90)):
        self.quality_range = quality_range

    def __call__(self, img):
        quality = random.randint(*self.quality_range)
        output_buffer = BytesIO()
        img.save(output_buffer, format='JPEG', quality=quality)
        output_buffer.seek(0)
        return Image.open(output_buffer)


# --- ã€æ–°å¢ã€‘Gate Annealing è°ƒåº¦å™¨ ---
def get_current_entropy_weight(epoch, initial_val=0.01, warmup_epochs=10, anneal_epochs=40):
    """
    è®¡ç®—å½“å‰ Epoch çš„ç†µæ­£åˆ™åŒ–æƒé‡
    ç­–ç•¥ï¼š
    1. Warm-up (0 ~ warmup_epochs): ä¿æŒé«˜æƒé‡ï¼Œå¼ºåˆ¶ç‰©ç†æµå‚ä¸ã€‚
    2. Annealing (warmup ~ warmup+anneal): çº¿æ€§è¡°å‡è‡³ 0ã€‚
    3. Free (warmup+anneal ~ end): 0 æƒé‡ï¼ŒGate è‡ªç”±å†³ç­–ã€‚
    """
    if epoch < warmup_epochs:
        return initial_val
    elif epoch < (warmup_epochs + anneal_epochs):
        # è¿›åº¦: 0.0 -> 1.0
        progress = (epoch - warmup_epochs) / float(anneal_epochs)
        # çº¿æ€§è¡°å‡: initial -> 0
        return initial_val * (1.0 - progress)
    else:
        return 0.0

def train():
    # --- 1. åŠ è½½é…ç½® ---
    # è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®
    config = yaml.safe_load(open("/root/autodl-tmp/detection/config/model_config.yaml", 'r', encoding='utf-8'))

    seed_everything(config['seed'])
    exp_name = config.get('exp_name') or f"exp_{time.strftime('%Y%m%d_%H%M%S')}_seed{config['seed']}"
    os.makedirs(f"{config['save_path']}/{exp_name}", exist_ok=True)
    logger = ExperimentLogger(log_dir=f"./{config['logs_path']}", experiment_name=exp_name)
    logger.log_hyperparams(config)

    # --- 2. æ•°æ®å‡†å¤‡ ---
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomApply([
            transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 3.0))
        ], p=0.1),
        transforms.RandomApply([
            RandomJPEGCompression(quality_range=(30, 100))
        ], p=0.1),
        transforms.ToTensor(),
        transforms.Normalize((0.4814, 0.4578, 0.4082), (0.2686, 0.2613, 0.2757))
    ])

    train_ds = ForensicDataset(root_dir='/root/autodl-tmp/data/train', transform=train_transform)
    val_ds = ForensicDataset(root_dir='/root/autodl-tmp/data/val', transform=train_transform)

    train_loader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, num_workers=4,
                              persistent_workers=True, pin_memory=True, worker_init_fn=worker_init_fn, drop_last=True)
    val_loader = DataLoader(val_ds, batch_size=config['batch_size'], shuffle=False, num_workers=4,
                            persistent_workers=True, pin_memory=True, worker_init_fn=worker_init_fn, drop_last=True)

    # --- 3. æ¨¡å‹åˆå§‹åŒ– ---
    model = TSFNet(config).to(config['device'])

    # --- 4. ä¼˜åŒ–å™¨ä¸æŸå¤±å‡½æ•° ---
    optimizer = optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=config['lr'],
        weight_decay=1e-2
    )

    # --- ã€å…³é”®ä¿®æ”¹ Aã€‘ åˆå§‹åŒ– CosineAnnealingLR ---
    # T_max: è®¾ä¸ºæ€» Epochsï¼Œè®©å­¦ä¹ ç‡åœ¨ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒå‘¨æœŸå†…ä¸‹é™åˆ° eta_min
    # eta_min: æœ€å°å­¦ä¹ ç‡ï¼Œé˜²æ­¢é™ä¸º 0ï¼Œå»ºè®®è®¾ä¸º 1e-6
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=config['epochs'],
        eta_min=1e-6
    )

    # --- æ–­ç‚¹ç»­è®­é€»è¾‘ ---
    resume_epoch = 0
    resume_path = f"{config['save_path']}/{exp_name}/model_epoch_{config['resume_epoch']}.pth"
    best_val_acc = 0.0

    if config['resume_epoch'] != resume_epoch and resume_path and os.path.exists(resume_path):
        logger.log_info(f"ğŸ”„ Resuming training from {resume_path}...")
        checkpoint = torch.load(resume_path, map_location=config['device'], weights_only=True)

        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        # æ³¨æ„ï¼šä¸¥æ ¼æ¥è¯´ï¼Œè¿™é‡Œä¹Ÿåº”è¯¥åŠ è½½ scheduler çš„çŠ¶æ€
        if 'scheduler_state_dict' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

        resume_epoch = checkpoint['epoch']
        best_val_acc = checkpoint.get('best_val_acc', 0.0)
        logger.log_info(f"ğŸ‘‰ Successfully loaded. Resuming from Epoch {resume_epoch + 1}")

    # åˆå§‹åŒ–æŸå¤±å‡½æ•°
    criterion_bce = torch.nn.BCEWithLogitsLoss()
    # criterion_supcon = SupConLoss(temperature=config['temp'])
    # criterion_orth = OrthogonalLoss()
    criterion_decorr = FineGrainedDecorrelationLoss()

    scaler = GradScaler('cuda')

    # --- 5. è®­ç»ƒå¾ªç¯ ---
    logger.log_info("ğŸš€ Start Training...")

    for epoch in range(resume_epoch, config['epochs']):
        model.train()
        train_loss = 0.0
        correct = 0
        total = 0
        base_entropy = config.get('lambda_entropy', 0.01)
        current_lambda_entropy = get_current_entropy_weight(epoch, initial_val=base_entropy, warmup_epochs=config.get('warmup_epochs', 3), anneal_epochs=config.get('anneal_epochs', 10))
        logger.log_file_only(f"Epoch [{epoch+1}] Annealing Status -> Lambda Entropy: {current_lambda_entropy:.6f}")
        # è·å–å½“å‰å­¦ä¹ ç‡ç”¨äºæ‰“å°
        current_lr = optimizer.param_groups[0]['lr']

        loop = tqdm(train_loader, leave=True)
        for batch_idx, (imgs, labels) in enumerate(loop):
            imgs, labels = imgs.to(config['device']), labels.to(config['device']).float()

            optimizer.zero_grad()

            with autocast('cuda'):
                # å‰å‘ä¼ æ’­
                logits, z_sem, _, f_sem_raw, v_forensic, alpha, f_tex_global, z_freq = model(imgs)

                # è®¡ç®—æŸå¤±
                loss_bce = criterion_bce(logits.squeeze(), labels)
                # loss_sc = criterion_supcon(z_sem, labels)
                # total_loss = loss_bce + config.get('lambda_supcon', 0) * loss_sc
                total_loss = loss_bce

                # loss_orth_val = 0.0
                loss_decorr_val = 0.0
                loss_entropy = 0.0
                if config.get('fusion_type') == 'gating':
                    # loss_orth_val = criterion_orth(f_sem_raw, v_forensic)
                    # total_loss += config.get('lambda_orth', 0.1) * loss_orth_val
                    loss_decorr_val = criterion_decorr(f_sem_raw.detach(), f_tex_global, z_freq)
                    total_loss += config.get('lambda_decorr', 0.01) * loss_decorr_val

                    if alpha is not None:
                        alpha_squeeze = alpha.squeeze()

                        # ã€æ ¸å¿ƒä¿®æ”¹ Bã€‘: ä» Max Entropy æ”¹ä¸º Margin Regularization
                        # é€»è¾‘ï¼šåªæƒ©ç½šæç«¯å€¼(0æˆ–1)ï¼Œå…è®¸ Alpha åœ¨ [0.5-delta, 0.5+delta] åŒºé—´å†…è‡ªç”±æ‘†åŠ¨
                        # delta = 0.35 æ„å‘³ç€å…è®¸åŒºé—´ä¸º [0.15, 0.85]
                        delta = 0.25

                        # è®¡ç®— Alpha åç¦» 0.5 çš„è·ç¦»
                        dist = torch.abs(alpha_squeeze - 0.5)

                        # åªæœ‰å½“åç¦»è¶…è¿‡ delta (å³è¿›å…¥æç«¯åŒºåŸŸ) æ—¶æ‰äº§ç”Ÿ Loss
                        # Loss = ReLU(dist - delta)
                        margin_loss = torch.relu(dist - delta)

                        loss_entropy = margin_loss.mean()

                        # ä½¿ç”¨é€€ç«æƒé‡ï¼šå‰æœŸå¼ºè¡ŒæŠŠ Alpha èµ¶å›ä¸­é—´ï¼ŒåæœŸæƒé‡å½’é›¶æ”¾é£ Gate
                        total_loss += current_lambda_entropy * loss_entropy
                # ---------------------------------------------------
                # ã€æ–°å¢ã€‘è®¡ç®— Gating Regularization Loss
                # ---------------------------------------------------
                loss_gate_val = 0.0

                # if config.get('fusion_type') == 'gating' and alpha is not None:
                #     # # 1. è®¡ç®—å‡å€¼å’Œæ–¹å·®
                #     # # alpha å½¢çŠ¶æ˜¯ [B, 1]ï¼Œå…ˆ squeeze æˆ [B]
                #     # alpha_squeeze = alpha.squeeze()
                #     #
                #     # alpha_mean = alpha_squeeze.mean()
                #     # alpha_var = alpha_squeeze.var()  # æ— åä¼°è®¡æ–¹å·®
                #     #
                #     # # 2. å®šä¹‰æƒé‡ (å¯ä»¥åœ¨ config é‡Œé…ï¼Œè¿™é‡Œå…ˆç¡¬ç¼–ç ç¤ºä¾‹)
                #     # # Î»_var: é¼“åŠ±æ–¹å·®å¤§ (è´Ÿå·åœ¨å…¬å¼é‡Œ)
                #     # # Î»_mean: é”šå®šå‡å€¼
                #     # lambda_var = 0.01  # è¿™æ˜¯ä¸€ä¸ªç»éªŒå€¼ï¼Œä¸è¦å¤ªå¤§ï¼Œå¦åˆ™æ¢¯åº¦ä¼šçˆ†
                #     # lambda_mean = 0.001  # "æå¼±"é”šå®š
                #     #
                #     # # 3. è®¡ç®—æŸå¤±
                #     # # æˆ‘ä»¬å¸Œæœ› Var å˜å¤§ -> Loss å˜å° -> -Var
                #     # loss_var_term = -alpha_var
                #     # loss_mean_term = (alpha_mean - 0.5) ** 2
                #     #
                #     # loss_gate_val = lambda_var * loss_var_term + lambda_mean * loss_mean_term
                #     #
                #     # # åŠ å…¥æ€»æŸå¤±
                #     # total_loss += config.get('lambda_gate', 0.01)* loss_gate_val
                #
                #     # 1. å–å‡º alpha [B, 1] -> [B]
                #     alpha_squeeze = alpha.squeeze()
                #
                #     # 2. è®¡ç®—ç†µ (Entropy)
                #     # H(p) = - [p*log(p) + (1-p)*log(1-p)]
                #     # åŠ  epsilon é˜²æ­¢ log(0)
                #     eps = 1e-8
                #     entropy = - (alpha_squeeze * torch.log(alpha_squeeze + eps) +
                #                  (1 - alpha_squeeze) * torch.log(1 - alpha_squeeze + eps))
                #
                #     # 3. è®¡ç®— Loss = - mean(Entropy)
                #     # æˆ‘ä»¬å¸Œæœ› Entropy æœ€å¤§ -> Loss æœ€å°
                #     loss_entropy = - entropy.mean()
                #
                #     # 4. åŠ æƒ
                #     # å»ºè®®æƒé‡ï¼š0.01 ~ 0.1ï¼Œå¦‚æœ Alpha ä¾ç„¶å¡åœ¨ 0.9ï¼Œå°±åŠ å¤§åˆ° 0.1
                #     # åŠ å…¥æ€» Loss
                #     total_loss += config.get('lambda_entropy', 0.001) * loss_entropy

            # åå‘ä¼ æ’­ä¸æ›´æ–°
            scaler.scale(total_loss).backward()
            scaler.step(optimizer)
            scaler.update()

            # ä¸ºäº†é˜²æ­¢å˜é‡æœªå®šä¹‰ï¼Œå…ˆè·å–æ•°å€¼ï¼ˆå®‰å…¨è·å–ï¼‰
            val_bce = loss_bce.item()
            # val_sc  = loss_sc.item()
            val_sc  = 0
            # val_orth = loss_orth_val.item() if isinstance(loss_orth_val, torch.Tensor) else 0.0
            val_decorr = loss_decorr_val.item() if isinstance(loss_decorr_val, torch.Tensor) else 0.0
            val_ent  = loss_entropy.item() if 'loss_entropy' in locals() and isinstance(loss_entropy, torch.Tensor) else 0.0

            # è®°å½•æ—¥å¿—
            global_step = epoch * len(train_loader) + batch_idx
            losses_dict = {
                'total': total_loss.item(),
                'bce': val_bce,
                # 'supcon': val_sc,
                'decorr': val_decorr,
                'entropy': val_ent,
            }
            logger.log_step(epoch, batch_idx, global_step, losses_dict)

            # ç»Ÿè®¡æŒ‡æ ‡
            train_loss += total_loss.item()
            preds = (torch.sigmoid(logits).squeeze() > 0.5).float()
            correct += (preds == labels).sum().item()
            total += labels.size(0)

            # æ˜¾å­˜ç›‘æ§ä¸æ‰“å°
            if batch_idx % 100 == 0:
                mem_alloc = torch.cuda.memory_allocated() / 1024 ** 3
                mem_res = torch.cuda.memory_reserved() / 1024 ** 3
                mem_peak = torch.cuda.max_memory_allocated() / 1024 ** 3
                torch.cuda.reset_peak_memory_stats()
                mem_info = f"Mem: {mem_alloc:.2f}G(A) / {mem_res:.2f}G(R) / {mem_peak:.2f}G(P)"
                # è¿™é‡ŒåŠ å…¥äº† LR çš„æ‰“å°
                loss_detail = f"Loss: {total_loss.item():.4f} [BCE:{val_bce:.4f} SC:{val_sc:.4f}"
                # å¦‚æœæœ‰ gating ç›¸å…³çš„æŸå¤±ï¼Œä¹Ÿæ‰“å°å‡ºæ¥
                if config.get('fusion_type') == 'gating':
                    loss_detail += f" Decorr:{val_decorr:.4f} Ent:{val_ent:.4f}"
                    if alpha is not None:
                        with torch.no_grad():
                            alpha_t = alpha.squeeze()  # [B]

                            # 0: Nature (Real), 1: AI (Fake)
                            mask_real = (labels == 0)
                            mask_fake = (labels == 1)

                            # è®¡ç®—å‡å€¼ (é˜²æ­¢ Batch ä¸­åªæœ‰ä¸€ç±»å¯¼è‡´é™¤ä»¥0)
                            mean_alpha_real = alpha_t[mask_real].mean().item() if mask_real.sum() > 0 else 0.0
                            mean_alpha_fake = alpha_t[mask_fake].mean().item() if mask_fake.sum() > 0 else 0.0

                            # æ·»åŠ åˆ°æ—¥å¿—å­—ç¬¦ä¸²
                            # Î±(R/F): Realå‡å€¼ / Fakeå‡å€¼
                            loss_detail += f" | Î±(R/F):{mean_alpha_real:.3f}/{mean_alpha_fake:.3f}"
                loss_detail += "]"
                logger.log_file_only(
                    f"Epoch [{epoch + 1}] Step [{batch_idx}] LR: {current_lr:.6f} | {loss_detail} | Acc: {correct / total:.4f} | {mem_info}"
                )
                logger.log_file_only(
                    f"Epoch [{epoch + 1}] Step [{batch_idx}] alpha: mean={alpha.mean().item():.4f}, min={alpha.min().item():.4f}, max={alpha.max().item():.4f}"
                )

            loop.set_description(f"Epoch [{epoch + 1}/{config['epochs']}]")
            loop.set_postfix(loss=total_loss.item(), acc=correct / total, lr=current_lr)

        # --- ã€å…³é”®ä¿®æ”¹ Bã€‘ åœ¨ Epoch ç»“æŸæ—¶æ›´æ–°å­¦ä¹ ç‡ ---
        scheduler.step()

        # --- 6. éªŒè¯ç¯èŠ‚ ---
        metrics = validate(model, val_loader, config, logger)
        logger.log_file_only(f"Epoch {epoch + 1} Val Acc: {metrics['Acc']:.4f}")

        # è®°å½• Epoch çº§æŒ‡æ ‡
        train_epoch_metrics = {'loss': train_loss / len(train_loader)}
        logger.log_epoch(epoch + 1, train_epoch_metrics, metrics, current_lr)

        # ä¿å­˜æ¨¡å‹
        current_save_path = f"{config['save_path']}/{exp_name}/model_epoch_{epoch + 1}.pth"
        checkpoint_dict = {
            'epoch': epoch + 1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),  # å»ºè®®åŠ å…¥è¿™ä¸ª
            'best_val_acc': best_val_acc
        }
        torch.save(checkpoint_dict, current_save_path)
        logger.log_file_only(f"Saved checkpoint to {current_save_path}")

        if metrics['Acc'] > best_val_acc:
            best_val_acc = metrics['Acc']
            logger.log_file_only(f"ğŸ”¥ New Best Model saved with Acc: {best_val_acc:.4f} at Epoch [{epoch + 1}]")


def validate(model, val_loader, config, logger):
    model.eval()
    evaluator = BinaryMetrics()

    # 1. åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
    sum_alpha_real = 0.0
    count_real = 0
    sum_alpha_fake = 0.0
    count_fake = 0

    with torch.no_grad():
        for imgs, labels in val_loader:
            imgs = imgs.to(config['device'])
            labels = labels.to(config['device']).float()

            # ä¿®æ”¹è§£åŒ…é€»è¾‘ï¼Œæ•è· alpha (ç¬¬6ä¸ªè¿”å›å€¼)
            # è¿”å›å€¼é¡ºåº: logits, z_sem, attn, f_sem, v_fore, alpha, f_tex, z_freq
            logits, _, _, _, _, alpha, _, _ = model(imgs)

            evaluator.update(logits.squeeze(), labels)

            # 2. ç»Ÿè®¡å½“å‰ Batch çš„ Alpha
            if alpha is not None:
                # è½¬ä¸º CPU numpy æ–¹ä¾¿è®¡ç®—
                alpha_np = alpha.squeeze().cpu().numpy()  # [B]
                labels_np = labels.cpu().numpy()  # [B]

                # å¤„ç† batch_size=1 çš„è¾¹ç¼˜æƒ…å†µ
                if alpha_np.ndim == 0:
                    alpha_np = np.array([alpha_np])

                # ç­›é€‰çœŸå›¾ (Label=0)
                mask_real = (labels_np == 0)
                if mask_real.any():
                    sum_alpha_real += alpha_np[mask_real].sum()
                    count_real += mask_real.sum()

                # ç­›é€‰å‡å›¾ (Label=1)
                mask_fake = (labels_np == 1)
                if mask_fake.any():
                    sum_alpha_fake += alpha_np[mask_fake].sum()
                    count_fake += mask_fake.sum()

    # 3. è®¡ç®—å¹¶æ‰“å°å…¨å±€å‡å€¼
    logger.log_file_only("-" * 30)  # åˆ†å‰²çº¿
    if count_real > 0 or count_fake > 0:
        avg_real = sum_alpha_real / count_real if count_real > 0 else 0.0
        avg_fake = sum_alpha_fake / count_fake if count_fake > 0 else 0.0
        logger.log_file_only(f"ğŸ§ [Val Gating Analysis]")
        logger.log_file_only(f"   >>> Mean Alpha (Real/Nature): {avg_real:.4f} (Should be Higher -> Trust CLIP)")
        logger.log_file_only(f"   >>> Mean Alpha (Fake/AI)    : {avg_fake:.4f} (Should be Lower  -> Trust Physics)")

    return evaluator.print_report()


if __name__ == "__main__":
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    train()
