import torch
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, precision_recall_fscore_support
from scipy.optimize import brentq
from scipy.interpolate import interp1d
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import os

# å¼•å…¥ä½ çš„æ¨¡å‹å’Œé…ç½®
from models.tsf_net import TSFNet
from data.dataset import ForensicDataset
# å‡è®¾ä½ æœ‰ä¸€ä¸ª get_val_transform ç”¨äºéªŒè¯é›†çš„é¢„å¤„ç†
from data.transforms import get_val_transform

def calculate_eer(y_true, y_score):
    """è®¡ç®— EER (Equal Error Rate)"""
    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)
    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)
    return eer

def evaluate(config):
    device = config['device']

    # 1. åŠ è½½æ¨¡å‹
    print(f"ğŸ”„ Loading model from {config['checkpoint_path']}...")
    model = TSFNet(config).to(device)
    checkpoint = torch.load(config['checkpoint_path'], map_location=device, weights_only=True)

    # å…¼å®¹å¤„ç†ï¼šæœ‰äº›ä¿å­˜å¯èƒ½æ˜¯æ•´åŒ…ï¼Œæœ‰äº›æ˜¯ state_dict
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)

    model.eval()

    # 2. å‡†å¤‡æ•°æ®
    test_dataset = ForensicDataset(
        root_dir=config['test_data_dir'],
        transform=get_val_transform(config['input_size'])
    )
    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4)

    # 3. æ¨ç†å¾ªç¯
    y_true = []
    y_scores = [] # è®°å½•æ¦‚ç‡å€¼
    y_preds = []  # è®°å½• 0/1 é¢„æµ‹ç»“æœ

    print("ğŸš€ Starting Evaluation...")
    with torch.no_grad():
        for imgs, labels in tqdm(test_loader):
            imgs = imgs.to(device)

            # å‡è®¾ä½ çš„ dataset è¿”å› label 0=Real, 1=Fake
            # ä½†æ¨¡å‹è¾“å‡ºé€šå¸¸æ˜¯ [B, 2] æˆ–è€… [B, 1]
            # è¿™é‡Œå‡è®¾æ¨¡å‹è¾“å‡º logits [B, 2]

            # éœ€è¦æ‰‹åŠ¨æå– clip_features æˆ–è€…ä¿®æ”¹æ¨¡å‹ forward é€»è¾‘
            # è¿™é‡Œç®€åŒ–æ¼”ç¤ºï¼Œå‡è®¾ model å†…éƒ¨å¤„ç†å¥½äº† clip é€»è¾‘ï¼Œæˆ–è€…ä½ éœ€è¦åƒ train.py ä¸€æ ·å…ˆè¿‡ clip
            # æ³¨æ„ï¼šå¦‚æœä½ çš„ model forward éœ€è¦ clip_embï¼Œè¿™é‡Œè¦è¡¥ä¸Š clip æå–ä»£ç 

            # --- ä¼ªä»£ç ï¼šå¦‚æœ model åŒ…å« clip é¢„å¤„ç† ---
            logits, _, _ = model(imgs)
            probs = torch.softmax(logits, dim=1)[:, 1] # å–å‡ºç±»åˆ« 1 (Fake) çš„æ¦‚ç‡

            preds = torch.argmax(logits, dim=1)

            y_true.extend(labels.cpu().numpy())
            y_scores.extend(probs.cpu().numpy())
            y_preds.extend(preds.cpu().numpy())

    # 4. è®¡ç®—æŒ‡æ ‡
    acc = accuracy_score(y_true, y_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_preds, average='binary')
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    roc_auc = auc(fpr, tpr)
    eer = calculate_eer(y_true, y_scores)

    cm = confusion_matrix(y_true, y_preds)

    print("\n" + "="*30)
    print(f"ğŸ“Š Evaluation Results:")
    print(f"Accuracy : {acc:.4f}")
    print(f"AUC      : {roc_auc:.4f}")
    print(f"EER      : {eer:.4f}") # è®ºæ–‡æ ¸å¿ƒæŒ‡æ ‡
    print(f"F1-Score : {f1:.4f}")
    print(f"Confusion Matrix:\n{cm}")
    print("="*30)

    # 5. ç»˜åˆ¶å¹¶ä¿å­˜ ROC æ›²çº¿ (å†™è®ºæ–‡ç”¨)
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc="lower right")
    plt.savefig('logs/roc_curve.png')
    print("ğŸ–¼ï¸ ROC Curve saved to logs/roc_curve.png")

if __name__ == "__main__":
    conf = {
        'device': 'cuda',
        'checkpoint_path': 'checkpoints/best_model.pth', # æŒ‡å‘ä½ è®­ç»ƒå¥½çš„æ¨¡å‹
        'test_data_dir': 'data/test',
        'batch_size': 64,
        'input_size': 224, # æ ¹æ®ä½ çš„ resize
        # ... å…¶ä»–æ¨¡å‹å‚æ•° ...
        'clip_model': "openai/clip-vit-base-patch32",
        'embed_dim': 256
    }
    evaluate(conf)
